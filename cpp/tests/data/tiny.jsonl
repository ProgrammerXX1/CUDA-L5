{"doc_id":"d001","external_id":"EXT-d001","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Это длинный тестовый документ для шингловой системы и поиска. Он нужен чтобы построить много шинглов k девять и проверить совпадения. Мы специально пишем несколько предложений подряд чтобы создать устойчивые последовательности токенов. В документе есть общая часть про индекс, сегмент и манифест. Также есть часть про чтение бинарного формата и проверку валидатором. Мы упоминаем postings и сортировку по hash did pos. Потом добавляем описание запроса и построения шинглов для query. После этого вставляем повторяемую фразу про простой тестовый документ для шингловой системы и поиска. В конце добавляем уникальные слова чтобы документ отличался от остальных.","text_is_normalized":true}
{"doc_id":"d002","external_id":"EXT-d002","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Это второй длинный документ который содержит похожие слова для теста поиска по шинглам. Он объясняет процесс построения сегмента и запись файлов index_native bin и docids json. Мы пишем что заголовок содержит magic PLAG и версию два. Затем добавляем что docmeta хранит tok_len и simhash. Потом описываем postings как тройки hash did pos. Далее говорим что сортировка postings обеспечивает детерминизм результатов. После этого упоминаем atomic replace tmp файлов и обновление manifest json. Затем добавляем абзац про многопоточность и объединение результатов потоков. Потом повторяем фразу что второй документ содержит похожие слова для теста поиска по шинглам. В конце добавляем уникальный хвост про стабильность и воспроизводимость тестов.","text_is_normalized":true}
{"doc_id":"d003","external_id":"EXT-d003","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Это третий документ который почти не пересекается с техническими описаниями индекса. Он всё равно достаточно длинный чтобы формировать шинглы и нагрузить билдер. Мы пишем про природу, горы, степи и сезонные изменения. Затем добавляем мысли про маршруты, поездки и расписания. Потом описываем как люди планируют дела и читают новости. Далее вставляем несколько предложений о том как важно избегать ложных совпадений. Потом упоминаем что k девять шинглы редко совпадают случайно на длинных текстах. Затем добавляем уникальные слова и выражения чтобы уменьшить пересечения. После этого пишем что результат поиска должен быть пустым для такого текста. Далее добавляем ещё одно предложение для длины и стабильности. В конце делаем вывод что документ служит отрицательным примером.","text_is_normalized":true}
{"doc_id":"d004","external_id":"EXT-d004","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ подробно про индексацию и построение postings. Сначала мы строим список токенов и проверяем что их достаточно для k девять. Затем для каждого окна длиной девять токенов считаем хэш шингла. После этого записываем тройки hash did pos в массив postings9. Затем сортируем postings по hash потом по did и потом по pos. Дальше объясняем что бинарный поиск lower_bound находит диапазон одинаковых hash. Потом говорим что это позволяет быстро собрать кандидатов по hits. Далее добавляем что на второй стадии строятся спаны по позициям pos. Потом описываем диагонали delta равное pos_doc минус pos_query. Затем повторяем ключевую фразу про сортировку postings по hash did pos. В конце добавляем уникальный хвост про будущую оптимизацию offsets и mmap.","text_is_normalized":true}
{"doc_id":"d005","external_id":"EXT-d005","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"В этом документе много технических терминов связанных с форматом и валидатором. Мы начинаем с описания header где есть PLAG и version два. Затем говорим что сохраняются N_docs и N_post9 и ноль для N_post13. Потом описываем блок метаданных где лежит tok_len и simhash. Далее упоминаем что docids json хранит отображение did в doc_id. Потом говорим что валидатор проверяет длину docids и сортировку postings. Затем добавляем проверку границ did и pos относительно tok_len. Далее описываем что запись по полям нужна чтобы избежать padding структур. Потом повторяем что формат должен быть совместимым на разных компиляторах. Затем добавляем ещё один абзац про контроль размеров файла и формулу header плюс docmeta плюс postings. В конце оставляем уникальную часть про checksum и контроль целостности.","text_is_normalized":true}
{"doc_id":"d006","external_id":"EXT-d006","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Это документ про тестовый корпус и то как его использовать для развития ядра. Сначала мы говорим что документы должны быть длинными и содержать десять предложений. Затем упоминаем что часть документов должна иметь общие блоки чтобы находились совпадения. Потом описываем что часть документов должна быть уникальной для проверки пустых результатов. Далее добавляем что при сборке билдер читает jsonl строки и парсит doc_id и text. Потом упоминаем что строгий режим нормализации зависит от переменной окружения. Затем говорим что после токенизации строятся шинглы и postings. Далее описываем что результаты потоков объединяются и сортируются. Потом упоминаем что поиск сначала считает hits а потом строит спаны. Затем добавляем что score вычисляется из coverage_query и coverage_doc. В конце вставляем уникальные слова про производительность и профилирование на больших данных.","text_is_normalized":true}
{"doc_id":"d007","external_id":"EXT-d007","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ о плагиате и о том как шинглы помогают его обнаружить. Сначала мы объясняем что совпадение последовательностей токенов даёт совпадение шинглов k девять. Затем говорим что одиночные совпадения не так важны как длинные цепочки. Потом описываем что спаны строятся по диагоналям на плоскости pos_query pos_doc. Далее говорим что span_min_len задаёт минимальную длину цепочки. Потом упоминаем что span_gap позволяет пропускать небольшие разрывы. Далее описываем что matched_shingles это сумма длин спанов. Потом говорим что coverage_query показывает долю запроса которая совпала. Затем добавляем что coverage_doc показывает долю документа которая совпала. Потом описываем что итоговый score это смесь по alpha. В конце делаем вывод что ядро должно быть строгим и устойчивым к шуму.","text_is_normalized":true}
{"doc_id":"d008","external_id":"EXT-d008","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про контроль качества и тестирование результата поиска. Сначала мы говорим что нужно иметь документы с общей частью и уникальным хвостом. Затем добавляем что общий блок должен давать длинные спаны а не случайные hits. Потом описываем что тесты должны быть детерминированными и не зависеть от текущей директории. Далее упоминаем что L5_TEST_DATA_DIR нужен чтобы находить tiny и большие корпуса. Потом говорим что assert в release может быть отключён поэтому нужны явные проверки. Затем описываем что поисковый тест должен брать query из документа чтобы гарантировать совпадение. Далее добавляем что min_hits и span_min_len влияют на чувствительность. Потом упоминаем что слишком частые шинглы нужно фильтровать как stop hashes. Затем добавляем что max_postings_per_hash защищает от мусорных совпадений. В конце оставляем уникальное предложение про будущие отчёты и подсветку совпадений.","text_is_normalized":true}
{"doc_id":"d009","external_id":"EXT-d009","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про производительность и параллельную сборку сегмента. Сначала мы описываем что билдер читает jsonl и держит строки в памяти. Затем говорим что каждый поток обрабатывает свой диапазон строк. Потом описываем что поток парсит json и извлекает doc_id и text. Далее говорим что если текст не нормализован то вызывается normalize_for_shingles_simple. Потом упоминаем что tokenize_spans строит список токенов. Затем говорим что shingle_stride задаёт шаг по позициям pos. Далее описываем что max_tokens_per_doc защищает от monster документов. Потом объясняем что postings9 собираются локально и потом did смещается base offset. Затем говорим что после merge postings сортируются глобально. В конце делаем вывод что такой подход даёт хорошую скорость на больших корпусах.","text_is_normalized":true}
{"doc_id":"d010","external_id":"EXT-d010","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"В этом документе мы повторяем базовую фразу чтобы создать общие шинглы. Сначала пишем что это простой тестовый документ для шингловой системы и поиска. Затем добавляем что это простой тестовый документ для шингловой системы и поиска ещё раз. Потом описываем что повторение помогает проверить ранжирование и topk. Далее говорим что при строгом поиске нужны спаны длиной не меньше нескольких шинглов. Потом добавляем что запрос должен содержать достаточно токенов чтобы построить много шинглов. Затем описываем что score зависит от coverage_query и coverage_doc. Далее говорим что такие документы помогают проверять partial совпадения. Потом упоминаем что валидатор должен проверять границы pos и did. Затем добавляем что поисковый движок должен быть стабильным и не падать. В конце добавляем уникальный хвост с редкими словами для различия между документами.","text_is_normalized":true}
{"doc_id":"d011","external_id":"EXT-d011","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ подробно описывает бинарный формат и требования совместимости. Сначала мы говорим что header содержит magic PLAG и версию два. Затем описываем что после header идут DocMeta записи по полям. Потом говорим что нельзя писать sizeof struct из за padding. Далее описываем что postings записываются как h did pos по полям. Потом упоминаем что docids хранится отдельно в json для удобства отладки. Затем добавляем что meta json хранит статистику docs и k9. Далее говорим что manifest json хранит список сегментов append only. Потом описываем что поиск читает manifest и сканирует сегменты. Затем добавляем что при повреждении сегмента поиск должен пропускать его без падений. В конце делаем вывод что формат должен оставаться стабильным при развитии ядра.","text_is_normalized":true}
{"doc_id":"d012","external_id":"EXT-d012","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про валидатор и защиту от повреждений. Сначала мы говорим что валидатор читает header и проверяет magic и версию. Затем добавляем что он проверяет что docids json имеет длину N_docs. Потом описываем что он проверяет сортировку postings по hash did pos. Далее говорим что он проверяет границы did и pos для каждого posting. Потом упоминаем что pos должен быть не больше tok_len минус k девять плюс один. Затем добавляем что можно расширить валидатор проверкой точного размера файла. Далее описываем что можно добавить checksum в meta и проверять его. Потом говорим что такой контроль уменьшает риск segfault в поиске. Затем упоминаем что ошибки должны быть понятными и возвращаться в json. В конце делаем вывод что валидатор это обязательная часть промышленной системы.","text_is_normalized":true}
{"doc_id":"d013","external_id":"EXT-d013","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ содержит семантически близкий текст к документу d002 для частичного совпадения. Сначала мы пишем что второй документ содержит похожие слова для теста поиска по шинглам. Затем добавляем описание процесса построения сегмента и сортировки postings. Потом говорим что после сборки нужно выполнить validate и убедиться что ok true. Далее описываем что поиск строит query шинглы и фильтрует слишком частые hash. Потом добавляем что затем выбираются кандидаты по hits и строятся спаны. Далее говорим что спаны учитывают delta и последовательность позиций. Потом описываем что score строится по coverage_query и coverage_doc. Затем повторяем ключевую фразу что второй документ содержит похожие слова для теста поиска по шинглам. Потом добавляем уникальный хвост про интеграцию с backend и subprocess. В конце делаем вывод что этот документ помогает проверить partial совпадения.","text_is_normalized":true}
{"doc_id":"d014","external_id":"EXT-d014","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ специально содержит множество общих фраз и повторов. Сначала мы пишем мы строим индекс и мы строим сегмент и мы строим postings. Затем повторяем что мы сортируем postings и затем ищем по postings. Потом добавляем что мы строим индекс и мы строим сегмент и мы строим postings снова. Далее описываем что такие повторения дают много одинаковых шинглов. Потом говорим что это позволяет проверить что topk выдаёт этот документ по запросу из общего блока. Затем добавляем что строгий span_min_len должен формировать длинные спаны на повторяющихся фразах. Далее говорим что score должен быть высоким из за большой доли совпадений. Потом упоминаем что stop hash фильтр не должен выкинуть все шинглы если текст слишком однообразный. Затем добавляем уникальную часть про будущий reranker или simhash. В конце делаем вывод что документ полезен для стресс теста.","text_is_normalized":true}
{"doc_id":"d015","external_id":"EXT-d015","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ описывает edge cases и предельные ситуации. Сначала мы говорим что документ может быть слишком коротким и тогда он не индексируется. Затем добавляем что документ может быть очень длинным и тогда он ограничивается max_tokens_per_doc. Потом описываем что число postings на документ может быть ограничено max_shingles_per_doc. Далее говорим что если doc_id пустой или text пустой то строка пропускается. Потом упоминаем что если json строка битая то парсер её пропускает. Затем добавляем что strict режим может трактовать отсутствие флага как не нормализованный текст. Далее говорим что если query слишком короткий то matches будет пустой и это нормально. Потом описываем что поиск не должен падать при пустом manifest и должен вернуть segments_scanned ноль. Затем добавляем что валидатор должен корректно сообщать что manifest пустой. В конце делаем вывод что такие случаи обязательны для промышленной устойчивости.","text_is_normalized":true}
{"doc_id":"d016","external_id":"EXT-d016","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ описывает работу query и базовый этап поиска. Сначала мы говорим что запрос нормализуется и токенизируется. Затем добавляем что из него строятся шинглы k девять и их hash значения. Потом описываем что для каждого hash делается lower_bound по postings. Далее говорим что диапазон одинаковых hash даёт множество did позиций. Потом упоминаем что hits считается по уникальным hash а не по всем позициям. Затем добавляем что после этого берутся topN кандидатов по hits. Далее говорим что только на кандидатах строятся spаны по pos. Потом описываем что matched_shingles считается как сумма длин спанов. Затем добавляем что score это alpha умножить coverage_query плюс остальное coverage_doc. В конце делаем вывод что такой пайплайн даёт баланс скорости и качества.","text_is_normalized":true}
{"doc_id":"d017","external_id":"EXT-d017","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про дальнейшее развитие и улучшение ядра поиска. Сначала мы говорим что после v1 нужно добавить более точное построение спанов. Затем добавляем что можно учитывать диагонали и плотность совпадений. Потом описываем что можно объединять пересекающиеся спаны и считать итоговую покрываемость без двойного счёта. Далее говорим что можно вводить штраф за разрывы и шум. Потом упоминаем что можно добавлять быстрый индекс offsets hash to range. Затем добавляем что можно хранить offsets в отдельном файле рядом с segment. Далее говорим что можно делать mmap чтение postings для скорости. Потом описываем что можно распараллелить поиск по сегментам. Затем добавляем что можно хранить метаданные сегментов в базе а сами файлы на диске. В конце делаем вывод что главное сохранить совместимость формата и тесты регрессии.","text_is_normalized":true}
{"doc_id":"d018","external_id":"EXT-d018","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Это документ на почти уникальную тему про железнодорожные процессы и регламенты. Сначала мы говорим про осмотр оборудования и оформление актов. Затем добавляем что существуют инструкции и процедуры проверки документов. Потом описываем что есть расписания и графики движения. Далее говорим что персонал должен соблюдать правила безопасности. Потом упоминаем что контроль качества включает отчёты и комиссионные проверки. Затем добавляем что данные часто хранятся в системах и требуют аудита. Далее говорим что эти тексты отличаются от описаний индексации и postings. Потом описываем что поиск по техническому запросу не должен находить этот документ. Затем добавляем что это полезно для проверки false positives. В конце делаем вывод что уникальные документы нужны в любом тестовом корпусе.","text_is_normalized":true}
{"doc_id":"d019","external_id":"EXT-d019","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про компиляцию и сборку проекта. Сначала мы пишем последовательность команд cmake минус S точка минус B build. Затем добавляем что нужно выполнить cmake --build build -j. Потом описываем что после сборки нужно запустить ctest из директории build. Далее говорим что тесты должны находить данные по абсолютному пути. Потом упоминаем что для этого используется compile definition L5_TEST_DATA_DIR. Затем добавляем что иначе test_build_smoke не найдёт tiny jsonl. Далее говорим что при изменении CMakeLists лучше делать чистую сборку. Потом описываем что ASAN помогает найти segfault и ошибки памяти. Затем добавляем что release может отключать assert и тесты должны быть устойчивыми. В конце делаем вывод что правильная инфраструктура сборки ускоряет развитие ядра.","text_is_normalized":true}
{"doc_id":"d020","external_id":"EXT-d020","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ содержит фразу для совпадений и дополнительные детали. Сначала мы пишем что это простой тестовый документ для шингловой системы и поиска. Затем добавляем что далее мы описываем сортировку postings и обновление manifest json. Потом говорим что каждая сборка создаёт новый сегмент и сохраняет его в out_root. Далее упоминаем что сегмент содержит index_native bin и docids json и meta json. Потом добавляем что поиск читает manifest и сканирует сегменты. Далее говорим что для каждого hash из query берётся диапазон postings и считается hits. Потом описываем что затем строятся спаны по позициям и вычисляется score. Затем повторяем фразу что это простой тестовый документ для шингловой системы и поиска. Потом добавляем уникальные слова про диагностику и логирование. В конце делаем вывод что документ пригоден для тестирования частичных совпадений.","text_is_normalized":true}
{"doc_id":"d021","external_id":"EXT-d021","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ похож на d002 и усиливает общую часть для совпадений. Сначала мы говорим что второй документ содержит похожие слова для теста поиска по шинглам. Затем добавляем что atomic replace tmp файлов важен для безопасности записи. Потом описываем что при записи сначала создаётся файл tmp и потом rename в финал. Далее говорим что если rename не получилось то удаляется финал и rename повторяется. Потом упоминаем что это best effort и на разных платформах атомарность разная. Затем добавляем что manifest обновляется append only и хранит список сегментов. Далее говорим что поиск должен корректно обрабатывать несколько сегментов. Потом описываем что merge выбирает лучший match по score. Затем повторяем что второй документ содержит похожие слова для теста поиска по шинглам. В конце добавляем уникальную часть про перенос манифеста в базу и хранение файлов на S3.","text_is_normalized":true}
{"doc_id":"d022","external_id":"EXT-d022","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про безопасность файлов и устойчивость записи. Сначала мы описываем что запись идёт через tmp и rename. Затем добавляем что это уменьшает риск получить частично записанный файл. Потом говорим что валидатор должен запускаться после сборки и перед поиском. Далее упоминаем что если валидатор нашёл ошибку то сегмент нельзя использовать. Потом добавляем что можно помечать сегмент как failed и не добавлять в manifest. Далее говорим что в будущем можно использовать файловые locks для конкурентных билдов. Потом описываем что при параллельных сборках нужно уникальное имя сегмента. Затем добавляем что имя сегмента может включать timestamp и pid. Потом говорим что мета должна сохранять дату построения и параметры поиска. В конце делаем вывод что надёжность записи критична для промышленного ядра.","text_is_normalized":true}
{"doc_id":"d023","external_id":"EXT-d023","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про многопоточность и структуру данных внутри билдера. Сначала мы говорим что каждый поток имеет локальные vectors docs и doc_ids и postings9. Затем добавляем что did внутри потока локальный и начинается с нуля. Потом описываем что после join мы вычисляем offsets по количеству docs на поток. Далее говорим что затем postings перезаписываются с did плюс base offset. Потом упоминаем что такой подход избавляет от блокировок на горячем пути. Затем добавляем что далее глобальные postings сортируются по hash did pos. Далее говорим что сортировка может быть дорогой на больших массивах. Потом описываем что для больших данных нужен spill merge и внешняя сортировка. Затем добавляем что пока тестовый корпус небольшой и in memory сортировки достаточно. В конце делаем вывод что архитектура сегментов позволяет масштабироваться.","text_is_normalized":true}
{"doc_id":"d024","external_id":"EXT-d024","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ содержит много повторов чтобы создать плотные совпадения. Сначала мы пишем мы строим индекс и проверяем индекс. Затем снова пишем мы строим индекс и проверяем индекс. Потом снова повторяем мы строим индекс и проверяем индекс. Далее добавляем что повтор создаёт последовательности токенов которые дают длинные спаны. Потом говорим что запрос содержащий эту фразу должен найти документ с высоким score. Затем описываем что spans должны быть длиннее span_min_len и тогда документ попадёт в результаты. Далее говорим что matched_shingles будет большим и coverage_query высокой. Потом добавляем что coverage_doc тоже будет заметной если документ состоит почти из повторов. Затем упоминаем что stop hash фильтр может сработать если шингл слишком частый по всему корпусу. В конце делаем вывод что повторяющиеся документы полезны для проверки устойчивости ядра.","text_is_normalized":true}
{"doc_id":"d025","external_id":"EXT-d025","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ описывает стратегию тестирования и оценку качества. Сначала мы говорим что нужен корпус из десятков документов для точных совпадений. Затем добавляем что нужны документы с частичным пересечением для проверки partial совпадений. Потом описываем что нужны уникальные документы для проверки пустых результатов. Далее говорим что важно чтобы каждый документ имел минимум девять токенов. Потом упоминаем что для строгого поиска со спанами нужны ещё более длинные документы. Затем добавляем что запросы для тестов должны браться из самих документов. Далее говорим что можно иметь отдельные тесты для strict и для smoke. Потом описываем что regression тесты должны ловить изменения в токенизации и нормализации. Затем добавляем что лучше фиксировать text_common и менять его только с reindex. В конце делаем вывод что качество ядра проверяется только на системных тестах.","text_is_normalized":true}
{"doc_id":"d026","external_id":"EXT-d026","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ на смешанную тему про c++ и python интеграцию. Сначала мы говорим что ядро индексирования и поиска выполняется на c++. Затем добавляем что backend может вызывать утилиты через subprocess. Потом описываем что это упрощает деплой и изоляцию ошибок. Далее говорим что позже можно перейти на shared library через pybind для меньшей задержки. Потом упоминаем что формат сегмента остаётся одинаковым и не зависит от способа вызова. Затем добавляем что метаданные сегментов можно хранить в postgres. Далее говорим что сами postings лучше хранить как файлы для mmap и page cache. Потом описываем что object storage тоже подходит если есть локальный кэш. Затем добавляем что важно иметь валидатор перед включением сегмента. В конце делаем вывод что архитектура должна быть простой и надёжной.","text_is_normalized":true}
{"doc_id":"d027","external_id":"EXT-d027","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про токенизацию и нормализацию и почему они важны. Сначала мы говорим что tokenize_spans в упрощённом варианте делит по пробелам. Затем добавляем что normalize_for_shingles_simple приводит текст к нижнему регистру и убирает пунктуацию. Потом описываем что это влияет на шинглы и на совпадения. Далее говорим что билдер и поиск должны использовать одну реализацию text_common. Потом упоминаем что иначе будет рассинхрон и поиск не найдёт совпадения. Затем добавляем что при обновлении text_common нужно поднимать версию формата или делать reindex. Далее говорим что для боевой версии нужна корректная utf8 обработка. Потом описываем что для тестов достаточно упрощённой схемы. Затем добавляем что важно иметь много предложений и достаточно токенов для k девять. В конце делаем вывод что quality ядра начинается с стабильной токенизации.","text_is_normalized":true}
{"doc_id":"d028","external_id":"EXT-d028","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ содержит частичное пересечение с d001 через общую фразу. Сначала мы пишем что это простой тестовый документ для шингловой системы и поиска. Затем добавляем что далее идёт другая часть про логирование и метрики. Потом описываем что логировать полезно segments_scanned и query_shingles_count. Далее говорим что также полезно логировать сколько stop hashes было отброшено. Потом упоминаем что это помогает понять почему matches пустой. Затем добавляем что ядро должно быть предсказуемым и детерминированным. Далее говорим что при одинаковом входе результаты должны быть одинаковыми. Потом описываем что ошибки должны возвращаться в json а не приводить к падениям. Затем добавляем что этот документ нужен для проверки partial score и коротких совпадений. В конце делаем вывод что общий блок и уникальный хвост создают хороший тестовый сценарий.","text_is_normalized":true}
{"doc_id":"d029","external_id":"EXT-d029","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про настройки и параметры поиска которые влияют на качество. Сначала мы описываем что plag_thr и partial_thr задают пороги классификации. Затем добавляем что alpha смешивает coverage_query и coverage_doc. Потом говорим что min_hits и span_min_len задают базовую чувствительность. Далее упоминаем что candidates_topn ограничивает число документов для дорогой стадии спанов. Потом добавляем что max_postings_per_hash фильтрует очень частые шинглы. Далее говорим что span_gap позволяет допускать небольшие разрывы в совпадениях. Потом описываем что max_spans_per_doc ограничивает размер ответа и время работы. Затем добавляем что эти параметры можно вынести в конфиг и хранить в meta сегмента. Потом говорим что тесты должны фиксировать ожидаемое поведение при разных параметрах. В конце делаем вывод что правильные параметры делают ядро практичным и быстрым.","text_is_normalized":true}
{"doc_id":"d030","external_id":"EXT-d030","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про jsonl формат корпуса и требования к данным. Сначала мы говорим что каждая строка должна быть отдельным json объектом. Затем добавляем что doc_id и text обязательны и должны быть строками. Потом описываем что text_is_normalized лучше задавать явно true для предсказуемости. Далее говорим что при strict режиме отсутствие флага трактуется как не нормализованный текст. Потом упоминаем что это может изменить токены и совпадения. Затем добавляем что битые строки можно пропускать чтобы процесс был устойчивым. Далее говорим что пустые тексты не индексируются. Потом описываем что слишком короткие тексты не дают k девять шинглов и тоже пропускаются. Затем добавляем что поэтому для тестов делаем длинные тексты из десяти предложений. В конце делаем вывод что качественные данные это основа качественного ядра.","text_is_normalized":true}
{"doc_id":"d031","external_id":"EXT-d031","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про данные и необходимость большого корпуса для тестирования. Сначала мы говорим что хотим корпус из пятидесяти документов по десять предложений. Затем добавляем что каждый документ должен быть длинным и содержать повторяемые блоки. Потом описываем что часть документов должна пересекаться через общие технические абзацы. Далее говорим что часть документов должна быть уникальной и не пересекаться. Потом упоминаем что это позволяет измерять precision и recall. Затем добавляем что для измерения нужно иметь предсказуемые запросы из документов. Далее говорим что можно сделать набор запросов и ожиданий как golden tests. Потом описываем что на каждой правке ядра нужно прогонять эти тесты. Затем добавляем что изменение токенизации или stop hash фильтра может изменить результаты. В конце делаем вывод что без большого корпуса сложно довести ядро до высокого уровня.","text_is_normalized":true}
{"doc_id":"d032","external_id":"EXT-d032","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про бинарный поиск и структуру postings массива. Сначала мы описываем что postings отсортированы по hash did pos. Затем добавляем что lower_bound быстро находит начало диапазона для hash. Потом говорим что дальнейший проход до изменения hash даёт все postings для данного шингла. Далее упоминаем что если диапазон слишком большой то это stop hash и его нужно игнорировать. Потом добавляем что это защищает от шинглов из частых слов и повторов. Далее говорим что на первой стадии hits считаются по уникальным hash запроса. Потом описываем что затем выбираются topN кандидатов и строятся спаны. Затем добавляем что спаны строятся по delta и последовательности позиций. Потом говорим что это даёт точный сигнал копирования. В конце делаем вывод что такая структура проста и эффективна.","text_is_normalized":true}
{"doc_id":"d033","external_id":"EXT-d033","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про позиционные совпадения и смысл поля pos. Сначала мы говорим что pos это позиция токена начала шингла в документе. Затем добавляем что pos_query это позиция шингла в запросе. Потом описываем что совпадение шингла даёт точку на плоскости pos_query pos_doc. Далее говорим что если текст скопирован то точки лежат на диагонали с постоянным delta. Потом упоминаем что мы группируем точки по delta и строим спаны. Затем добавляем что спан это непрерывная последовательность точек по pos_query и pos_doc. Далее говорим что span_min_len задаёт минимальный размер совпадения. Потом описываем что matched_shingles это сумма длин спанов и это уже хороший сигнал. Затем добавляем что по этим спанам можно позже извлекать текстовые отрывки из оригинала. В конце делаем вывод что pos критичен для анти плагиата.","text_is_normalized":true}
{"doc_id":"d034","external_id":"EXT-d034","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про стабильность чтения и записи и проблемы padding. Сначала мы говорим что структуры в c++ могут иметь padding и разный sizeof. Затем добавляем что поэтому нельзя писать бинарник как dump структуры. Потом описываем что нужно писать и читать поля по отдельности. Далее говорим что это гарантирует совместимость между компиляторами и флагами. Потом упоминаем что валидатор должен ловить любые несостыковки по counts и размерам. Затем добавляем что можно проверять точный размер файла чтобы обнаружить обрезания. Далее говорим что если формат изменится нужно увеличить версию и сделать миграцию. Потом описываем что в промышленной системе лучше избегать скрытых зависимостей от ABI. Затем добавляем что текущая архитектура сегментов облегчает reindex потому что каждый сегмент независим. В конце делаем вывод что стабильность формата важнее всего при развитии ядра.","text_is_normalized":true}
{"doc_id":"d035","external_id":"EXT-d035","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ уникален и посвящён разговорам о путешествиях и погоде. Сначала мы описываем утро в городе и планы на день. Затем добавляем что люди смотрят прогноз и выбирают маршрут. Потом говорим что можно поехать в горы или остаться дома. Далее упоминаем что в степи зимой холодно а летом жарко. Потом добавляем что такие тексты не должны совпадать с техническими описаниями индекса. Далее говорим что поиск по запросу про postings должен не находить этот документ. Потом описываем что если документ вдруг попадает в matches значит stop hash фильтр или токенизация слишком слабые. Затем добавляем что такие отрицательные примеры помогают настроить пороги. Потом говорим что k девять снижает вероятность случайных совпадений. В конце делаем вывод что разнообразие корпуса улучшает оценку качества ядра.","text_is_normalized":true}
{"doc_id":"d036","external_id":"EXT-d036","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ имеет общую часть с d002 и d013 и предназначен для пересечений. Сначала мы говорим что второй документ содержит похожие слова для теста поиска по шинглам. Затем добавляем что мы описываем обновление manifest и создание директории сегмента. Потом говорим что build создаёт seg_dir и пишет три файла индекса. Далее упоминаем что validate проверяет docids и сортировку postings. Потом добавляем что search читает manifest и запускает search_in_segment. Далее говорим что в ядре есть stage A hits и stage B spans. Потом описываем что score вычисляется из coverage_query и coverage_doc. Затем добавляем что этот документ должен находиться по запросу содержащему общий блок. Потом повторяем фразу что второй документ содержит похожие слова для теста поиска по шинглам. В конце добавляем уникальный хвост про деплой и эксплуатацию.","text_is_normalized":true}
{"doc_id":"d037","external_id":"EXT-d037","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про логику сегментов и режим append only. Сначала мы говорим что каждый запуск билдера создаёт новый сегмент. Затем добавляем что сегмент сохраняется в out_root и не изменяется. Потом описываем что manifest хранит список сегментов и их статистику. Далее говорим что поиск читает manifest и сканирует все сегменты. Потом упоминаем что на больших данных можно сканировать только активные сегменты. Затем добавляем что сегменты можно позже объединять компакцией или слиянием. Далее говорим что пока мы фокусируемся на корректности и качестве ядра. Потом описываем что если сегмент повреждён он должен быть пропущен. Затем добавляем что валидатор помогает обнаружить проблему заранее. В конце делаем вывод что сегментная архитектура даёт простую масштабируемость.","text_is_normalized":true}
{"doc_id":"d038","external_id":"EXT-d038","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про тесты и правильный выбор запроса. Сначала мы говорим что smoke тест должен строить сегмент и затем делать поиск. Затем добавляем что запрос должен быть достаточно длинным для k девять. Потом описываем что если запрос короткий то query_shingles пустой и matches пустой. Далее говорим что для строгого поиска со spанами нужно ещё больше токенов. Потом упоминаем что лучше брать запрос прямо из документа tiny jsonl. Затем добавляем что тогда совпадение гарантировано и тест устойчив. Далее говорим что test_search_smoke должен проверять что matches не пустой и не падать. Потом описываем что в release assert отключён поэтому тест должен возвращать код ошибки. Затем добавляем что failure должен печатать понятную диагностику. В конце делаем вывод что качественные тесты ускоряют развитие ядра.","text_is_normalized":true}
{"doc_id":"d039","external_id":"EXT-d039","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ намеренно повторяет общую фразу несколько раз для совпадений. Сначала мы пишем это простой тестовый документ для шингловой системы и поиска. Затем ещё раз пишем это простой тестовый документ для шингловой системы и поиска. Потом добавляем что такая повторяемость создаёт много одинаковых шинглов. Далее говорим что при запросе из этой фразы будут найдены длинные спаны. Потом упоминаем что score будет высоким и matched_shingles большим. Затем добавляем что документ должен быть в topk результатов. Далее говорим что если он не найден значит проблема в токенизации или в stop hash фильтре. Потом описываем что этот документ помогает ловить регрессии в ядре. Затем добавляем уникальный хвост про логирование и метрики. В конце делаем вывод что повторяемые документы полезны для отладки.","text_is_normalized":true}
{"doc_id":"d040","external_id":"EXT-d040","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про метрики и наблюдаемость системы. Сначала мы говорим что meta json может хранить docs и post9 и параметры поиска. Затем добавляем что built_at_utc помогает отслеживать свежесть сегмента. Потом описываем что search может возвращать segments_scanned и количество matches. Далее говорим что полезно логировать query_shingles_count и число кандидатов. Потом упоминаем что также полезно логировать число stop hashes и диапазоны postings. Затем добавляем что такие метрики позволяют улучшать производительность. Далее говорим что по метрикам можно видеть регрессии после правок ядра. Потом описываем что в будущем можно добавить профилирование и тайминги стадий. Затем добавляем что вывод json должен быть стабильным для интеграции с backend. В конце делаем вывод что наблюдаемость помогает довести ядро до высокого уровня.","text_is_normalized":true}
{"doc_id":"d041","external_id":"EXT-d041","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про мердж результатов и возможные оптимизации хранения. Сначала мы описываем что после работы потоков docs и doc_ids объединяются в один массив. Затем добавляем что postings пересобираются с поправкой did и сортируются. Потом говорим что для больших данных лучше не держать всё в памяти и делать spill на диск. Далее упоминаем что можно строить несколько сегментов и искать по всем сегментам. Потом добавляем что это проще чем один монолитный индекс на раннем этапе. Далее говорим что позднее можно добавить компакцию сегментов в один большой. Потом описываем что поиск multi segment уже работает через manifest. Затем добавляем что merge по doc_id выбирает лучший score. Потом говорим что можно учитывать дату сегмента или приоритет. В конце делаем вывод что текущая архитектура гибкая и пригодна для масштабирования.","text_is_normalized":true}
{"doc_id":"d042","external_id":"EXT-d042","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про разнообразие лексики и устойчивость к случайным совпадениям. Сначала мы пишем про обучение, математику и алгоритмы. Затем добавляем про распределение памяти и работу кэша. Потом описываем про сортировку и бинарные границы lower_bound. Далее говорим что даже при разных темах могут встречаться общие слова. Потом упоминаем что k девять снижает влияние общих коротких фраз. Затем добавляем что stop hash фильтр убирает слишком частые шинглы. Далее говорим что spаны требуют последовательность и поэтому случайные совпадения отсекаются. Потом описываем что для оценки надо иметь отрицательные документы и случайные запросы. Затем добавляем что такие документы помогают настроить пороги plag_thr и partial_thr. В конце делаем вывод что разнообразие корпуса важно для качества ядра.","text_is_normalized":true}
{"doc_id":"d043","external_id":"EXT-d043","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про отображение did в doc_id и компактность postings. Сначала мы говорим что did это просто индекс документа в сегменте. Затем добавляем что docids json хранит список doc_id в этом порядке. Потом описываем что postings содержит только did и pos что экономит память. Далее говорим что при поиске мы возвращаем doc_id а did остаётся внутренним. Потом упоминаем что это позволяет легко объединять результаты и строить отчёты. Затем добавляем что при нескольких сегментах один doc_id может встречаться несколько раз. Далее говорим что merge выбирает лучший score и сохраняет segment_name источника. Потом описываем что так можно реализовать версионность документов. Затем добавляем что позже можно добавить хранение исходного текста чтобы показывать совпавшие отрывки. В конце делаем вывод что такое разделение делает систему практичной.","text_is_normalized":true}
{"doc_id":"d044","external_id":"EXT-d044","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про будущую оптимизацию offsets и ускорение range_for_hash. Сначала мы описываем что сейчас для каждого hash делается lower_bound по postings. Затем добавляем что это логарифмическая операция и на длинных query может быть дорогой. Потом говорим что можно построить offsets таблицу hash to range и хранить её рядом с индексом. Далее упоминаем что тогда поиск будет O один для перехода к диапазону. Потом добавляем что offsets можно строить на этапе сборки сегмента. Далее говорим что размер offsets зависит от числа уникальных hash в postings. Потом описываем что можно сжать offsets или хранить только для часто используемых hash. Затем добавляем что при mmap чтении postings можно ускорить поиск через page cache. Потом говорим что такие оптимизации нужны после доведения корректности и качества. В конце делаем вывод что offsets это следующий этап производительности.","text_is_normalized":true}
{"doc_id":"d045","external_id":"EXT-d045","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про промышленный контроль и режим validate. Сначала мы говорим что validate должен проверять не только сортировку но и размеры файла. Затем добавляем что он должен проверять что counts соответствуют содержимому. Потом описываем что он должен проверять что docids json валиден и не содержит пустых строк. Далее говорим что он должен проверять что pos не выходит за пределы tok_len минус k. Потом упоминаем что такие проверки предотвращают segfault в поиске. Затем добавляем что при ошибке сегмент не должен использоваться и не должен попадать в активный набор. Далее говорим что можно хранить статус сегмента и показывать его в ui. Потом описываем что промышленная система должна иметь журнал ошибок и метрики. Затем добавляем что тесты должны включать сценарии повреждённого сегмента. В конце делаем вывод что контроль качества обязателен для ядра.","text_is_normalized":true}
{"doc_id":"d046","external_id":"EXT-d046","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про строгую нормализацию и влияние флага text_is_normalized. Сначала мы говорим что при strict режиме отсутствие флага означает что текст не нормализован. Затем добавляем что тогда применяется normalize_for_shingles_simple. Потом описываем что если текст уже нормализован то повторная нормализация может изменить токены. Далее говорим что лучше хранить флаг на этапе ETL и передавать его в корпус. Потом упоминаем что в тестах мы ставим text_is_normalized true чтобы исключить влияние окружения. Затем добавляем что в проде можно включать strict чтобы не доверять данным. Далее говорим что это уменьшает риск мусорных токенов и странных совпадений. Потом описываем что любые изменения нормализации требуют reindex. Затем добавляем что поэтому нормализация должна быть стабильной и хорошо протестированной. В конце делаем вывод что строгий режим повышает надёжность но требует дисциплины данных.","text_is_normalized":true}
{"doc_id":"d047","external_id":"EXT-d047","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про прогресс разработки и то что уже сделано. Сначала мы говорим что проект l5_engine собран и тесты проходят. Затем добавляем что мы исправили проблемы с путями тестовых данных. Потом описываем что мы добавили автономный text_common для сборки. Далее говорим что мы исправили запись бинарника по полям чтобы избежать padding. Потом упоминаем что мы добавили в поиск этапы hits и spans и новый score. Затем добавляем что теперь результат поиска может содержать spans и coverage. Далее говорим что следующий шаг это улучшение качества и оптимизация производительности. Потом описываем что нужен большой корпус и набор регрессионных запросов. Затем добавляем что нужно настроить параметры span_min_len и stop hash фильтра. В конце делаем вывод что ядро развивается правильно и его надо довести до высокого уровня.","text_is_normalized":true}
{"doc_id":"d048","external_id":"EXT-d048","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ похож на d004 и содержит много совпадающих технических фраз. Сначала мы описываем что мы строим список токенов и затем считаем хэш каждого окна k девять. Затем добавляем что мы записываем postings и сортируем их по hash did pos. Потом говорим что это создаёт пересечение шинглов между документами. Далее упоминаем что поиск находит диапазон одинаковых hash через lower_bound. Потом добавляем что дальше считаются hits по did и выбираются кандидаты. Далее говорим что затем строятся spаны по delta и последовательности pos. Потом описываем что score вычисляется из coverage_query и coverage_doc. Затем добавляем что этот документ должен быть найден по запросу содержащему часть описания индексации. Потом говорим что такие документы помогают проверить что search возвращает несколько результатов с разным score. В конце делаем вывод что пересечения между d004 и d048 нужны для проверки ранжирования.","text_is_normalized":true}
{"doc_id":"d049","external_id":"EXT-d049","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про детерминизм и воспроизводимость результатов. Сначала мы говорим что сортировка postings делает индекс детерминированным. Затем добавляем что при одинаковом входе build выдаёт одинаковый bin файл. Потом описываем что это важно для regression тестов и сравнения результатов. Далее говорим что если результаты меняются значит изменилась токенизация или параметры. Потом упоминаем что такие изменения нужно фиксировать через версию и миграцию. Затем добавляем что для поиска важно стабильное ранжирование topk. Далее говорим что если два документа имеют одинаковый score можно сортировать по matched_shingles и hits. Потом описываем что это даёт устойчивый порядок. Затем добавляем что для сравнения результатов удобно сохранять golden output json. В конце делаем вывод что детерминизм ускоряет отладку и развитие ядра.","text_is_normalized":true}
{"doc_id":"d050","external_id":"EXT-d050","organization_id":"org_demo","source_path":"uploads/org_demo/corpus_50_long.jsonl","text":"Этот документ про чтение сегмента и роль reader слоя. Сначала мы говорим что reader открывает index_native bin и читает header. Затем добавляем что reader читает docmeta и postings по полям и проверяет поток чтения. Потом описываем что затем reader читает docids json и возвращает массив строк. Далее говорим что search_multi использует reader для каждого сегмента. Потом упоминаем что если reader не может прочитать сегмент он возвращает ошибку и сегмент пропускается. Затем добавляем что это повышает устойчивость поиска и не даёт падать на битых файлах. Далее говорим что валидатор использует reader чтобы проверить сортировку postings и диапазоны did pos. Потом описываем что в будущем reader можно оптимизировать через mmap. Затем добавляем что mmap позволит быстрее читать postings и использовать page cache. В конце делаем вывод что reader это важный слой для промышленной системы.","text_is_normalized":true}
