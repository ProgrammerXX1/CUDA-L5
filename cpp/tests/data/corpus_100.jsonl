{"doc_id":"d001","text":"это простой тестовый документ для шингловой системы и поиска; он содержит достаточно слов чтобы образовать много k=9 шинглов, и в нём есть повторяющиеся фразы для проверки совпадений, пересечений и частичных совпадений между документами","text_is_normalized":true}
{"doc_id":"d002","text":"второй документ содержит похожие слова для теста поиска по шинглам; здесь мы добавляем длинное описание процесса построения сегмента, сортировки postings и дальнейшего чтения индекса чтобы обеспечить стабильный результат и воспроизводимость тестов","text_is_normalized":true}
{"doc_id":"d003","text":"совсем другой текст который почти не пересекается; однако он всё равно достаточно длинный, чтобы формировать шинглы и позволять движку отрабатывать нормальные случаи без совпадений, проверяя что ранжирование не выдаёт ложные срабатывания","text_is_normalized":true}
{"doc_id":"d004","text":"документ про индексацию: мы строим список токенов, затем для каждого окна длиной девять токенов считаем хэш, записываем (hash,did,pos) и сортируем по hash потом по did и pos; это помогает быстро искать диапазон одинаковых hash в postings","text_is_normalized":true}
{"doc_id":"d005","text":"в этом тексте много технических терминов: сегмент, манифест, бинарный формат, заголовок PLAG версия 2, количество документов и количество postings; далее идёт блок метаданных и массив postings k9; всё это проверяется валидатором на консистентность","text_is_normalized":true}
{"doc_id":"d006","text":"описание тестового корпуса: мы хотим чтобы документы были длинными, содержали повторяемые фразы и разные темы; например про компиляцию, cmake, сборку, запуск l5_build, l5_validate и l5_search, и про то как query превращается в шинглы","text_is_normalized":true}
{"doc_id":"d007","text":"документ о плагиате: если два текста совпадают по последовательностям токенов, то у них совпадут шинглы k=9; чем больше совпадений и чем плотнее они по позициям pos, тем выше вероятность заимствования; позже мы добавим построение спанов","text_is_normalized":true}
{"doc_id":"d008","text":"контроль качества: полезно иметь тексты с общей частью и уникальным хвостом; общий блок используется чтобы проверить обнаружение совпадений, а уникальный хвост проверяет что алгоритм не путает разные документы; также важно иметь документы без общих фраз","text_is_normalized":true}
{"doc_id":"d009","text":"проверка производительности: даже на небольшом корпусе мы хотим увидеть корректную работу многопоточности; поток читает строки jsonl, парсит doc_id и text, решает нужно ли нормализовать, токенизирует и генерирует postings; затем результаты мерджатся","text_is_normalized":true}
{"doc_id":"d010","text":"в этом документе повторяем базовую фразу: это простой тестовый документ для шингловой системы и поиска; мы повторим её ещё раз чтобы увеличить вероятность совпадений; это простой тестовый документ для шингловой системы и поиска; дальше добавим уникальные слова","text_is_normalized":true}
{"doc_id":"d011","text":"описание формата: header включает magic PLAG, version=2, N_docs, N_post9, N_post13; затем для каждого документа записывается tok_len и simhash 128; затем postings9 как тройки (h,did,pos); чтение делаем по полям чтобы избежать padding","text_is_normalized":true}
{"doc_id":"d012","text":"документ о валидаторе: он проверяет что docids.json имеет длину N_docs, что postings отсортированы по (h,did,pos), что did не выходит за диапазон документов, и что pos не превышает tok_len-k; это позволяет ловить повреждения файлов до поиска","text_is_normalized":true}
{"doc_id":"d013","text":"пример семантически близкого текста: второй документ содержит похожие слова для теста поиска по шинглам и для проверки совпадений; здесь мы добавляем похожие фразы про сборку сегмента и работу манифеста чтобы получить частичное пересечение","text_is_normalized":true}
{"doc_id":"d014","text":"пример текста с общими фразами: мы строим индекс, мы строим сегмент, мы строим postings, затем сортируем, затем ищем; такие повторения делают много одинаковых шинглов и помогают проверить что topk ранжирование возвращает ожидаемые документы","text_is_normalized":true}
{"doc_id":"d015","text":"проверка edge cases: документ может быть очень длинным, тогда мы ограничиваем max_tokens_per_doc и max_shingles_per_doc; документ может быть слишком коротким, тогда он не индексируется; документ может иметь пустой doc_id или пустой text, тогда строка пропускается","text_is_normalized":true}
{"doc_id":"d016","text":"в тексте описываем работу query: запрос нормализуется, токенизируется, из него строятся шинглы k=9; для каждого шингла делаем binary search в postings по hash и считаем попадания в did; пока score равен hits делённое на количество шинглов запроса","text_is_normalized":true}
{"doc_id":"d017","text":"документ о дальнейшем развитии: нужно добавить построение спанов, то есть группировать совпадения по pos, учитывать допустимый разрыв span_gap, минимальную длину цепочки span_min_len и ограничение max_spans_per_doc; потом вычислять plagiarism score по alpha и thresholds","text_is_normalized":true}
{"doc_id":"d018","text":"пример почти уникального текста: здесь обсуждаются железнодорожные процессы, осмотр оборудования, проверка документов, инструкции и регламенты; текст длинный и разнообразный, вероятность совпадения с техническими описаниями индексации минимальна; это проверка на ложные совпадения","text_is_normalized":true}
{"doc_id":"d019","text":"документ про компиляцию: cmake генерирует build, затем cmake --build собирает библиотеку и утилиты, после чего ctest прогоняет тесты; важно чтобы тестовые данные находились по абсолютному пути, иначе при запуске из build директории относительные пути ломаются","text_is_normalized":true}
{"doc_id":"d020","text":"текст с фразой для совпадений: это простой тестовый документ для шингловой системы и поиска; затем добавим дополнительные слова чтобы было больше токенов и больше окон длиной девять; потом опишем сортировку postings и обновление manifest json","text_is_normalized":true}
{"doc_id":"d021","text":"второй документ содержит похожие слова для теста поиска по шинглам; здесь мы повторяем ту же идею, но добавляем другие детали: atomic replace tmp файлов, проверка существования сегмента, формирование имени сегмента и запись meta json с stats","text_is_normalized":true}
{"doc_id":"d022","text":"документ про безопасность файлов: atomic replace best effort сначала пытается rename tmp в финал, если не получилось удаляет финал и снова rename; это не идеально атомарно на всех платформах, но достаточно для локального fs; в проде можно усиливать через versioned файлы","text_is_normalized":true}
{"doc_id":"d023","text":"текст про многопоточность: каждый поток имеет локальные vectors docs, doc_ids, postings9; did внутри потока локальный, потом мы вычисляем offsets и прибавляем base; так мы избегаем синхронизации на горячем пути и получаем линейный мердж после join потоков","text_is_normalized":true}
{"doc_id":"d024","text":"документ с большим количеством повторов: мы строим индекс и проверяем индекс; мы строим индекс и проверяем индекс; мы строим индекс и проверяем индекс; повтор создаёт множество одинаковых шинглов и проверяет что поиск возвращает этот документ при похожем запросе","text_is_normalized":true}
{"doc_id":"d025","text":"описание тестовой стратегии: нужен корпус из десятков и сотен документов; часть документов должна иметь общие блоки текста, часть должна быть полностью разной; тогда можно проверить precision и recall; важно чтобы все документы имели минимум девять токенов","text_is_normalized":true}
{"doc_id":"d026","text":"документ на смешанную тему: разработка на c++ и python, интеграция через subprocess, хранение сегментов на диске, манифест в json; позже можно перенести метаданные сегментов в postgres, но бинарные postings лучше оставить файлами для mmap и page cache","text_is_normalized":true}
{"doc_id":"d027","text":"текст про токены: токенизация в упрощённом text_common разделяет по пробелам; это достаточно для тестов; в боевой версии можно заменить на более строгую utf8 токенизацию и нормализацию; главное чтобы билдер и поиск использовали одну и ту же реализацию","text_is_normalized":true}
{"doc_id":"d028","text":"пример документа с частичным пересечением: это простой тестовый документ для шингловой системы и поиска, но далее идёт другая часть про логирование, метрики и обработку ошибок; совпадёт только начало, и это должно дать частичный score после внедрения спанов","text_is_normalized":true}
{"doc_id":"d029","text":"пример текста о настройках: параметры thresholds plag_thr и partial_thr определяют границы; alpha определяет вклад разных сигналов; w_min_doc и w_min_query задают минимальные размеры; пока это только мета, но потом будет участвовать в ранжировании и фильтрации","text_is_normalized":true}
{"doc_id":"d030","text":"документ про jsonl: каждая строка должна быть отдельным json объектом; поля doc_id и text обязательны; text_is_normalized предпочтительно, иначе при strict=1 отсутствие флага трактуется как не нормализованный текст и включается normalize_for_shingles_simple; это влияет на совпадения","text_is_normalized":true}
{"doc_id":"d031","text":"длинный документ о данных: мы хотим увеличить корпус до ста документов; каждый документ должен быть достаточно длинным, содержать несколько предложений, технические термины и повторяемые блоки; это позволит тестировать производительность, корректность и устойчивость к шуму","text_is_normalized":true}
{"doc_id":"d032","text":"текст про бинарный поиск: postings отсортированы по hash; для каждого hash из query мы ищем lower_bound и далее идём вперёд пока hash совпадает; каждый posting даёт did, мы увеличиваем hits; затем сортируем кандидатов по score и отдаём topk; это базовый движок без спанов","text_is_normalized":true}
{"doc_id":"d033","text":"документ про позиционные совпадения: поле pos — это позиция токена начала шингла; когда добавим спаны, мы будем сопоставлять pos_doc и pos_query, искать диагонали и непрерывные последовательности; это поможет выделять скопированные фрагменты и строить отчёт","text_is_normalized":true}
{"doc_id":"d034","text":"текст о стабильности: важно чтобы чтение и запись были совместимыми; мы пишем поля по отдельности, а не sizeof(struct), чтобы не зависеть от padding; это особенно критично при смене компилятора или флагов; валидатор должен ловить любые несостыковки по размерам","text_is_normalized":true}
{"doc_id":"d035","text":"пример уникального документа: разговор о погоде, поездках, расписаниях и маршрутах; он длинный, но в нём нет терминов про сегменты и postings; такой документ нужен чтобы система не находила совпадений там где их нет; при запросах про индекс он не должен попадать в topk","text_is_normalized":true}
{"doc_id":"d036","text":"текст с общей частью: второй документ содержит похожие слова для теста поиска по шинглам; дополнительно здесь есть описание обновления manifest и создания директории сегмента; такая общая часть должна пересекаться с несколькими документами и проверять merge результатов по сегментам","text_is_normalized":true}
{"doc_id":"d037","text":"документ про логику сегментов: каждый запуск билдера создаёт отдельный сегмент; манифест хранит список сегментов; поиск читает манифест и сканирует сегменты; в будущем можно включать и выключать сегменты, делать компакцию и слияние, но базовый режим append-only уже работает","text_is_normalized":true}
{"doc_id":"d038","text":"пример текста о тестах: smoke тест строит сегмент, затем валидирует и ищет; важно чтобы query имел минимум девять токенов, иначе шинглы не строятся; также важно чтобы tiny.jsonl содержал достаточно токенов в каждом документе, иначе документы будут пропускаться билдером","text_is_normalized":true}
{"doc_id":"d039","text":"документ с повторяемой фразой: это простой тестовый документ для шингловой системы и поиска; это простой тестовый документ для шингловой системы и поиска; дальше идут разные детали про сборку, валидацию и поиск; повторяющаяся часть помогает поиску уверенно находить этот документ","text_is_normalized":true}
{"doc_id":"d040","text":"текст о данных и метриках: в meta json мы можем сохранять статистику docs, k9, k13; также можно сохранять параметры поиска; это облегчает диагностику и воспроизводимость; в будущем можно добавить checksum файлов и версию генератора для контроля совместимости","text_is_normalized":true}
{"doc_id":"d041","text":"документ про параллельный мердж: после работы потоков мы объединяем docs и doc_ids в глобальные массивы, а postings9 пересобираем с поправкой did; затем сортируем; если позже добавим spill на диск, можно будет избегать хранения всех postings в памяти; пока простой режим достаточен","text_is_normalized":true}
{"doc_id":"d042","text":"пример текста с разной лексикой: обучение, математика, алгоритмы, распределение памяти, mmap, page cache, сортировка, нижняя и верхняя граница; даже если словарь разный, некоторые общие фразы могут пересекаться; это проверка устойчивости к случайным совпадениям на k=9","text_is_normalized":true}
{"doc_id":"d043","text":"документ про формирование doc_id: doc_id это строковый идентификатор исходного документа; в индексе did это просто порядковый номер; docids.json хранит отображение did->doc_id; при поиске мы возвращаем doc_id, а did остаётся внутренним; это позволяет хранить компактные postings в бинарнике","text_is_normalized":true}
{"doc_id":"d044","text":"текст про будущее улучшение: чтобы ускорить поиск, можно построить вспомогательный индекс hash->range (offsets) и хранить его рядом, чтобы не делать lower_bound каждый раз; но даже без этого на маленьком корпусе всё работает; на больших корпусах offsets даст большой прирост","text_is_normalized":true}
{"doc_id":"d045","text":"пример документа о системах контроля: в промышленной системе важно иметь режим validate, который проверяет не только сортировку, но и размеры файла, и корректность counts; можно также проверять что postings хэшируются в допустимом диапазоне; всё это уменьшает риск падений при поиске","text_is_normalized":true}
{"doc_id":"d046","text":"документ о параметрах строгой нормализации: при PLAGIO_STRICT_TEXT_IS_NORMALIZED=1 отсутствие флага означает что текст не нормализован и будет прогнан через normalize; при 0 наоборот; для тестов лучше всегда явно ставить text_is_normalized:true чтобы не зависеть от окружения","text_is_normalized":true}
{"doc_id":"d047","text":"длинный документ на тему разработки: мы собрали проект l5_engine, добавили text_common, исправили формат и тесты, теперь build validate search проходят; дальше усиливаем ядро в c++ и добавляем реальные плагиат метрики; важно сохранять обратную совместимость формата сегмента","text_is_normalized":true}
{"doc_id":"d048","text":"пример текста с почти теми же словами что и в документе d004: мы строим список токенов, затем считаем хэш каждого окна из девяти токенов, записываем postings и сортируем; это создаёт пересечение шинглов и позволяет проверять что поиск возвращает несколько документов с близким score","text_is_normalized":true}
{"doc_id":"d049","text":"документ про стабильные результаты: сортировка postings обеспечивает детерминизм; при одинаковом входе build создаёт одинаковый набор postings и одинаковый порядок; это важно для тестов и для отладки; если позже добавим рандомизацию, нужно будет контролировать seed и воспроизводимость","text_is_normalized":true}
{"doc_id":"d050","text":"пример текста про чтение файла: reader открывает index_native.bin, читает header, читает docmeta и postings по полям; затем отдельно читает docids.json; если что-то не совпадает, валидатор выдаёт ошибку; такой слой упрощает поиск и изоляцию проблем формата","text_is_normalized":true}
{"doc_id":"d051","text":"текст на тему поиска: бинарный поиск по postings для каждого hash может быть дорогим при большом query; можно дедуплицировать hash в query и считать веса; можно считать частоты; можно ограничивать слишком частые hash; позже это можно добавить чтобы улучшить качество и скорость","text_is_normalized":true}
{"doc_id":"d052","text":"документ о пересечениях: если два документа имеют общий абзац, то множество шинглов будет совпадать; score по hits даст высокую оценку; но чтобы отличить случайные совпадения от реального копирования, нужны спаны и плотность по pos; это следующий этап ядра","text_is_normalized":true}
{"doc_id":"d053","text":"пример текста без общих фраз: здесь описываются рецепты, ингредиенты, кухонные процессы, способы приготовления и тайминги; в таком документе не должно быть совпадений с текстами про индекс; это контроль на false positives и на устойчивость к случайным совпадениям в k=9","text_is_normalized":true}
{"doc_id":"d054","text":"документ про память: postings9 могут занимать много; каждый posting это 16 байт на диске; при ста миллионах postings это гигабайты; поэтому важны ограничения max_shingles_per_doc и возможный spill merge; но на текущем этапе мы строим небольшой корпус для проверки ядра","text_is_normalized":true}
{"doc_id":"d055","text":"текст про логирование ошибок: l5_build должен возвращать понятные ошибки если не может открыть корпус, создать директорию или записать файлы; l5_validate должен показывать список ошибок; l5_search должен корректно обрабатывать пустой query и отсутствующие сегменты без падений","text_is_normalized":true}
{"doc_id":"d056","text":"пример текста с общей фразой: это простой тестовый документ для шингловой системы и поиска; затем идёт обсуждение алгоритмов и структуры данных; такая комбинация даёт как совпадающие шинглы, так и уникальные; позже спаны покажут что совпала только часть документа","text_is_normalized":true}
{"doc_id":"d057","text":"документ о компоновке: лучше иметь библиотеку l5_engine и отдельные утилиты; backend вызывает утилиты через subprocess или через shared library; пока subprocess проще; при необходимости low latency можно перейти к pybind; формат сегмента остаётся тем же","text_is_normalized":true}
{"doc_id":"d058","text":"пример длинного текста о документах: документ может содержать юридические термины, технические регламенты и внутренние инструкции; если один и тот же раздел встречается в разных файлах, система должна это видеть; шинглы k=9 хорошо подходят для детекта буквальных совпадений","text_is_normalized":true}
{"doc_id":"d059","text":"документ про тестовый набор: мы хотим 100 документов; часть будет техническими и похожими, часть будет уникальными; если запрос взят из технической части, то topk должны быть технические документы; если запрос из уникальной части, совпадений может не быть; это тоже корректно","text_is_normalized":true}
{"doc_id":"d060","text":"документ про стабильность токенизации: если токенизация меняется, меняются шинглы; поэтому text_common должен быть единственным источником истины; нельзя чтобы билдер и поиск использовали разные версии; если нужно обновить, делаем versioning формата или reindex всех сегментов","text_is_normalized":true}
{"doc_id":"d061","text":"пример текста про сегменты и манифест: сегмент хранит индекс, а манифест список сегментов; при поиске мы сканируем сегменты и берём лучший match по doc_id; пока merge простой; в будущем можно учитывать свежесть сегмента или приоритет; это добавится в search_multi","text_is_normalized":true}
{"doc_id":"d062","text":"документ с повторяющимися словами: поиск поиск поиск индексация индексация индексация сегмент сегмент сегмент; такие повторы могут дать много одинаковых токенов и повлиять на шинглы; это тест на то что система не ломается на однообразных текстах и корректно считает шинглы","text_is_normalized":true}
{"doc_id":"d063","text":"текст о проверке pos: pos это индекс токена, а не символа; при совпадениях мы будем использовать pos чтобы строить непрерывные цепочки; если текст короткий и имеет только один шингл, pos всегда 0; если длинный, pos растёт; валидатор проверяет pos<=tok_len-9","text_is_normalized":true}
{"doc_id":"d064","text":"документ про обработку json: simdjson быстро парсит строки; если строка битая, её можно пропустить; doc_id и text должны быть строками; если text пустой, документ пропускается; это позволяет обрабатывать большие корпуса устойчиво без падений на единичных ошибках","text_is_normalized":true}
{"doc_id":"d065","text":"пример текста с частичным совпадением с d001: это простой тестовый документ для шингловой системы и поиска; далее в этом документе идёт другая тема про оптимизацию и кеширование; общая часть даёт попадания, а остальное уникально; хорошая база для будущей логики спанов","text_is_normalized":true}
{"doc_id":"d066","text":"документ про конфиг: сейчас meta содержит пример конфигурации; позже можно сделать отдельный index_config.json который читает поиск и использует параметры; тогда билдер будет записывать cfg_hash; это позволит иметь разные режимы на разных сегментах и корректно воспроизводить результаты поиска","text_is_normalized":true}
{"doc_id":"d067","text":"текст про binary format: важно сохранять little endian и фиксированные размеры типов; мы используем uint32 и uint64; если переносить на другую платформу, нужно следить за совместимостью; запись по полям решает padding, но endian всё ещё нужно учитывать если будет big endian платформа","text_is_normalized":true}
{"doc_id":"d068","text":"пример документа о наблюдаемости: полезно печатать docs и post9 после сборки; также полезно логировать время построения и размер файлов; для поиска полезно логировать segments_scanned, query_shingles_count и top scores; это поможет оптимизировать ядро и находить регрессии","text_is_normalized":true}
{"doc_id":"d069","text":"документ про качество детекта: k=9 хорошо ловит почти буквальные совпадения; если текст перефразирован, шинглы будут совпадать мало; тогда нужны другие сигналы, например simhash или embeddings; но для уровня 5 задача именно быстро ловить высокую буквальную схожесть","text_is_normalized":true}
{"doc_id":"d070","text":"пример текста про сортировку: мы сортируем postings по hash, затем did, затем pos; такой порядок нужен для бинарного поиска диапазона по hash и для последующей группировки по did; если позже добавим compressed postings или offsets, сортировка остаётся базовым требованием формата","text_is_normalized":true}
{"doc_id":"d071","text":"документ с общей фразой и дополнительными деталями: это простой тестовый документ для шингловой системы и поиска; мы обсуждаем как писать и читать поля, как избегать segfault в тестах, как правильно прокидывать пути через CMake; это даёт пересечения с другими техническими документами","text_is_normalized":true}
{"doc_id":"d072","text":"пример текста про concurrency: multi-thread парсинг jsonl ускоряет сборку; но запись файла делается одним потоком после мерджа; это упрощает атомарность и формат; если корпус огромный, можно сделать streaming с spill; пока мы концентрируемся на корректности ядра и тестах","text_is_normalized":true}
{"doc_id":"d073","text":"документ на другую тему: финансовые отчёты, бюджетирование, закупки, таблицы, поставщики, тендеры; текст длинный, но в нём нет повторов из индексных описаний; он нужен чтобы проверить что поиск по техническому запросу не вытаскивает нерелевантные документы","text_is_normalized":true}
{"doc_id":"d074","text":"пример текста про работу с файлами: сегмент создаётся в отдельной папке; если папка уже существует, билдер падает; это защищает от случайной перезаписи; если нужен режим update, он должен быть отдельным флагом; в текущем режиме append-only сегменты накапливаются и не меняются","text_is_normalized":true}
{"doc_id":"d075","text":"документ про тестовый запрос: для smoke поиска запрос лучше брать кусок из одного документа; тогда гарантированно будет совпадение; если запрос слишком короткий, шинглы не строятся; если запрос слишком общий, может совпасть с несколькими документами; это полезно для проверки topk и score","text_is_normalized":true}
{"doc_id":"d076","text":"пример текста про checksum: можно добавить sha256 для index_native.bin и docids.json и хранить в meta; валидатор будет проверять checksum; это защитит от частичных записей и повреждений; пока достаточно проверок counts и сортировки, но checksum усилит надёжность на проде","text_is_normalized":true}
{"doc_id":"d077","text":"документ про будущую оптимизацию: вместо json docids можно хранить бинарный блок строк или отдельный mmap файл; json удобен для отладки, но медленнее; на больших корпусах лучше бинарный формат; однако для текущего этапа отладки и тестов json подходит","text_is_normalized":true}
{"doc_id":"d078","text":"пример текста с пересечением со словами из d002: второй документ содержит похожие слова для теста поиска по шинглам; тут мы повторяем похожие выражения и добавляем детали про поисковый процесс; это должно дать несколько совпадений и показать что score может быть меньше чем 1 при большем запросе","text_is_normalized":true}
{"doc_id":"d079","text":"документ о правильной обработке ошибок: поиск не должен падать если сегмент отсутствует или повреждён; он должен пропускать сегмент и продолжать; для этого reader возвращает false и ошибку; search_multi должен корректно продолжать; текущая реализация уже пропускает нечитабельные сегменты","text_is_normalized":true}
{"doc_id":"d080","text":"пример текста про тесты в release: assert может быть отключён; поэтому тесты должны использовать явные проверки и возвращать код ошибки, а не полагаться на assert; мы исправили test_search_smoke именно так; это повышает надёжность тестов в разных режимах сборки","text_is_normalized":true}
{"doc_id":"d081","text":"документ про сборку: последовательность команд cmake -S -B, затем cmake --build, затем ctest; это должно быть воспроизводимо; если меняется CMakeLists, лучше делать чистый build; это уменьшает риск того что старые бинарники тестов будут использовать старые пути и падать","text_is_normalized":true}
{"doc_id":"d082","text":"пример текста про ограничения: max_tokens_per_doc и max_shingles_per_doc защищают от monster-docs; если документ огромный, мы обрезаем токены и ограничиваем postings; это сохраняет предсказуемость времени сборки и размера сегмента; позже можно делать sampling или adaptive stride","text_is_normalized":true}
{"doc_id":"d083","text":"документ про merge результатов: сейчас search_multi выбирает лучший match по doc_id среди сегментов; если один и тот же doc_id встречается в разных сегментах, берётся лучший score; позже можно учитывать дату сегмента или версию; также можно возвращать segment_name источника лучшего совпадения","text_is_normalized":true}
{"doc_id":"d084","text":"пример текста про детект перекрытий: если совпадения шинглов идут подряд по pos, это сильный сигнал; если совпадения редкие и разбросаны, это слабый сигнал; при реализации спанов мы будем оценивать плотность и длину непрерывных цепочек; пока hits даёт только грубый счётчик","text_is_normalized":true}
{"doc_id":"d085","text":"документ про сортировку и стабильность: после мерджа postings сортируются; это может быть тяжёлой операцией; для больших массивов нужна внешняя сортировка или spill merge; пока в памяти; но архитектура сегментов позволяет позже строить несколько маленьких сегментов и объединять их на уровне поиска","text_is_normalized":true}
{"doc_id":"d086","text":"пример документа с другой тематикой: история, культура, литература, музыка, события; текст длинный и разнообразный; он нужен чтобы статистически проверить что случайные совпадения k=9 редки; если вдруг появляются совпадения, значит нормализация слишком агрессивная или токены слишком короткие","text_is_normalized":true}
{"doc_id":"d087","text":"документ про унификацию: все компоненты — билдер, валидатор, поиск — используют один формат и один text_common; это уменьшает вероятность расхождений; любые изменения должны сопровождаться тестами; если формат меняется, нужно увеличить version и поддерживать чтение старых версий или делать reindex","text_is_normalized":true}
{"doc_id":"d088","text":"пример текста про диагностику mismatch: если docs меньше ожидаемого, значит часть документов слишком короткая или пустая; если post9 слишком мало, значит токенов около девяти или мало строк; можно увеличить тексты, добавить больше слов и предложений; это улучшит тестирование и приближает к боевым условиям","text_is_normalized":true}
{"doc_id":"d089","text":"документ про построение большого корпуса: можно сгенерировать сто документов с разными шаблонами; часть будет содержать общий технический абзац, часть — разные уникальные блоки; таким образом можно проверять что поиск находит общую часть, а также что не путает разные темы; это база для профилирования","text_is_normalized":true}
{"doc_id":"d090","text":"пример текста с общей фразой: это простой тестовый документ для шингловой системы и поиска; далее идёт описание корректности, проверок, и как система обрабатывает utf8; такой документ пересекается с несколькими и должен быть найден по запросу, содержащему этот общий фрагмент","text_is_normalized":true}
{"doc_id":"d091","text":"документ про параметры normalized: если text_is_normalized=true, билдер не вызывает нормализацию; если false, вызывает; это влияет на токены и хэши; в тестовых данных мы ставим true для предсказуемости; в проде можно хранить флаг на этапе ETL и контролировать его строгость через env","text_is_normalized":true}
{"doc_id":"d092","text":"пример текста про выход search: сейчас возвращаем doc_id, hits и score; позже добавим spans с позициями и длинами; затем python обёртка сможет вытаскивать исходные отрывки из текста по токенным позициям через отдельный хранилище; это позволит строить отчёт о плагиате с подсветкой совпадений","text_is_normalized":true}
{"doc_id":"d093","text":"документ о масштабировании: когда сегментов станет много, поиск может сканировать только последние или только активные; можно хранить в манифесте флаг active; можно хранить в базе и выбирать сегменты через sql; но ядро поиска по сегменту должно оставаться быстрым и простым; поэтому важно довести его до высокого уровня","text_is_normalized":true}
{"doc_id":"d094","text":"пример текста о корректности чтения: reader должен строго читать количество записей по counts; если файл короче, чтение провалится; если файл длиннее, можно игнорировать хвост или считать ошибкой; валидатор может проверять точный размер файла по формуле header+docmeta+postings; это улучшит надёжность","text_is_normalized":true}
{"doc_id":"d095","text":"документ про корректность тестов: тесты должны быть детерминированными и не зависеть от текущей директории; поэтому мы прокидываем L5_TEST_DATA_DIR через compile definitions; это позволяет запускать ctest из build; также полезно печатать диагностические сообщения при ошибках, чтобы быстро видеть причину","text_is_normalized":true}
{"doc_id":"d096","text":"пример текста с большим числом токенов: здесь мы добавляем много слов подряд чтобы увеличить количество шинглов и нагрузку на postings; это поможет проверить что сортировка и поиск работают на больших массивах; также это позволит проверить что score перестаёт быть 1.0 и становится дробным при более длинном запросе","text_is_normalized":true}
{"doc_id":"d097","text":"документ на тему системного программирования: memory layout, alignment, padding, запись по полям, ошибки линковки, asan, ubsan, gdb; всё это встречалось при сборке; такой документ пересекается с техническими текстами и полезен для проверки совпадений в наборе из ста документов","text_is_normalized":true}
{"doc_id":"d098","text":"пример текста с минимальным пересечением: описание природы, гор, степи, климата и сезонных изменений; он длинный, но не содержит общих технических фраз; по запросу о postings и сегментах этот документ не должен появляться; это контроль качества и отрицательный пример","text_is_normalized":true}
{"doc_id":"d099","text":"документ про диагностику query: если query слишком короткий и шинглов нет, поиск должен вернуть пустой matches без падения; это корректно; в ui можно показать сообщение что запрос слишком короткий для k=9; в тестах мы избегаем этого и используем длинный запрос; это улучшает стабильность","text_is_normalized":true}
{"doc_id":"d100","text":"финальный документ набора: это простой тестовый документ для шингловой системы и поиска; добавим длинный хвост с описанием построения сегментов, чтения индекса, валидации формата, и поиска по postings; этот документ должен хорошо находиться по запросу, содержащему общий технический абзац и несколько уникальных слов","text_is_normalized":true}
